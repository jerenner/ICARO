{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal vs. background classification in NEXT\n",
    "\n",
    "In this notebook we read in the prepared data, construct and train the DNN, and then evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrenner/miniconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import h5py\n",
    "import matplotlib        as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import random            as rd\n",
    "import tables            as tb\n",
    "import tensorflow        as tf\n",
    "import pandas            as pd\n",
    "\n",
    "from glob                import glob\n",
    "from matplotlib.patches  import Ellipse\n",
    "from __future__          import print_function\n",
    "\n",
    "# Keras imports\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.models               import Model, load_model\n",
    "from keras.layers               import Input, Dense, MaxPooling3D, AveragePooling3D, Conv3D, Conv2D, AveragePooling2D, Activation, Dropout, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers           import SGD, Adam, Nadam         \n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core          import Flatten\n",
    "from keras                      import callbacks\n",
    "from keras.regularizers         import l2, l1\n",
    "from keras.initializers         import RandomNormal\n",
    "from keras.utils.layer_utils    import print_summary\n",
    "\n",
    "mpl.rcParams.update({'font.size': 14})\n",
    "mpl.rcParams['image.cmap'] = 'Greys'\n",
    "mpl.rcParams['patch.force_edgecolor'] = False\n",
    "mpl.rcParams['patch.facecolor'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable definitions\n",
    "Here we define key variables to be used throughout the notebook.  Note that we will read the data from the files stored in the directories `data_train_sg` and `data_train_bg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dimensions\n",
    "xdim = 30\n",
    "ydim = 30\n",
    "zdim = 30\n",
    "\n",
    "# voxel size (in mm)\n",
    "vox_size = [15, 15, 15]\n",
    "\n",
    "# range of expected events (in one dimension)\n",
    "rng = 450\n",
    "\n",
    "# data location and training/test file numbers\n",
    "data_train_sg = \"/data/detsim/true/15x15x15-1FWHM/EPEM\"\n",
    "data_train_bg = \"/data/detsim/true/15x15x15-1FWHM/SE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "### Data input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read events from the specified list of files until a number of events nevts is reached.\n",
    "# - construct and return a 3D voxel matrix and the corresponding vector of event energies\n",
    "def read_evts(files, nevts):\n",
    "    \n",
    "    arr_voxels = []\n",
    "    arr_energies = []\n",
    "    arr_evtnum = []\n",
    "    \n",
    "    evt_count = 0\n",
    "    for fn in files:\n",
    "        \n",
    "        print(\"Reading file {}\".format(fn))\n",
    "        \n",
    "        if(evt_count >= nevts):\n",
    "            break\n",
    "        \n",
    "        # Get the voxel table.\n",
    "        tf = tb.open_file(fn,'r')\n",
    "        vtbl = tf.root.TrueVoxels.Voxels\n",
    "        \n",
    "        # Get the energies.\n",
    "        events = np.concatenate((vtbl[np.nonzero(np.diff(vtbl[:]['event'],1))[0]]['event'],[vtbl[-1]['event']]))\n",
    "        for evt in events:\n",
    "            \n",
    "            if(evt_count >= nevts):\n",
    "                break\n",
    "            \n",
    "            # Get the indices in the table for voxels of this event number.\n",
    "            evt_indices = vtbl[:]['event'] == evt\n",
    "            \n",
    "            # Get the voxel coordinates and energy.\n",
    "            vox_X = vtbl[evt_indices]['X']\n",
    "            vox_Y = vtbl[evt_indices]['Y']\n",
    "            vox_Z = vtbl[evt_indices]['Z']\n",
    "            vox_E = vtbl[evt_indices]['E']\n",
    "            \n",
    "            # Center the voxels.\n",
    "            xmin = np.min(vox_X); xmax = np.max(vox_X)\n",
    "            ymin = np.min(vox_Y); ymax = np.max(vox_Y)\n",
    "            zmin = np.min(vox_Z); zmax = np.max(vox_Z)\n",
    "            x0 = ((xmin + xmax) / 2. - (rng / 2.))\n",
    "            y0 = ((ymin + ymax) / 2. - (rng / 2.))\n",
    "            z0 = ((zmin + zmax) / 2. - (rng / 2.))\n",
    "\n",
    "            # Create the voxel array.\n",
    "            varr = np.zeros([xdim, ydim, zdim])\n",
    "\n",
    "            # Iterate through the voxels, applying offsets.\n",
    "            valid_voxels = True\n",
    "            for vX,vY,vZ,vE in zip(vox_X, vox_Y, vox_Z, vox_E):\n",
    "                \n",
    "                if(not valid_voxels): break\n",
    "                \n",
    "                ivox = int((vX - x0)/vox_size[0])\n",
    "                jvox = int((vY - y0)/vox_size[1])\n",
    "                kvox = int((vZ - z0)/vox_size[2])\n",
    "                evox = vE\n",
    "                if(ivox < 0 or ivox >= xdim or jvox < 0 or jvox >= ydim or kvox < 0 or kvox >= zdim):\n",
    "                    print(\"WARNING: event {} out of range\".format(evt))\n",
    "                    valid_voxels = False\n",
    "                else:\n",
    "                    #if(varr[ivox][jvox][kvox] > 0.):\n",
    "                    #    print(\"WARNING: duplicate voxel in event {}\".format(evt))\n",
    "                    varr[ivox][jvox][kvox] += vE\n",
    "                    \n",
    "            # Normalize the voxel array.\n",
    "            norm = np.sum(varr)\n",
    "            if(norm > 0):\n",
    "                varr /= norm\n",
    "            \n",
    "            # Save the voxel matrix and energy sum.\n",
    "            if(valid_voxels):\n",
    "                arr_voxels.append(varr)\n",
    "                arr_energies.append(np.sum(vox_E))\n",
    "                arr_evtnum.append(evt)\n",
    "                evt_count += 1\n",
    "    \n",
    "    return arr_voxels, arr_energies, arr_evtnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to read the data from multiple files.\n",
    "def read_MC(path_sg, path_bg, nevts, fval):\n",
    "    \"\"\"Reads all events from the files in the specified directories, with nevts max of each type (signal and background)\"\"\"\n",
    "    \n",
    "    # Read in the signal events.\n",
    "    files = glob(path_sg + '/*h5')\n",
    "    files = sorted(files, key=lambda s: int(s.split('_')[1]))\n",
    "    s_array, s_energies, s_evtnum = read_evts(files,nevts)\n",
    "    \n",
    "    # Read in the background events.\n",
    "    files = glob(path_bg + '/*h5')\n",
    "    files = sorted(files, key=lambda s: int(s.split('_')[1]))\n",
    "    b_array, b_energies, b_evtnum = read_evts(files,nevts)\n",
    "        \n",
    "    # concatenate the datasets, splitting into training and validation sets\n",
    "    print(\"Concatenating datasets...\")\n",
    "    nval = int(fval/2 * (len(s_array) + len(b_array)))\n",
    "    \n",
    "    if(nval == 0):\n",
    "        x_ = np.concatenate([s_array, b_array])\n",
    "        y_ = np.concatenate([np.ones([len(s_array), 1]), np.zeros([len(b_array), 1])])\n",
    "\n",
    "        # reshape for training with TensorFlow        \n",
    "        print(\"Reshaping projection...\")\n",
    "        x_ = np.reshape(x_, (len(x_), xdim, ydim, zdim, 1))\n",
    "        print(\"Finished reading data: {0} training/test events\".format(len(x_)))\n",
    "        return x_,y_\n",
    "    else:\n",
    "        x_ = np.concatenate([s_array[0:-nval], b_array[0:-nval]])\n",
    "        y_ = np.concatenate([np.ones([len(s_array[0:-nval]), 1]), np.zeros([len(b_array[0:-nval]), 1])])\n",
    "        xval_ = np.concatenate([s_array[-nval:], b_array[-nval:]])\n",
    "        yval_ = np.concatenate([np.ones([len(s_array[-nval:]), 1]), np.zeros([len(b_array[-nval:]), 1])])\n",
    "\n",
    "        # reshape for training with TensorFlow\n",
    "        print(\"Reshaping projection...\")\n",
    "        x_ = np.reshape(x_, (len(x_), xdim, ydim, zdim, 1))\n",
    "        xval_ = np.reshape(xval_, (len(xval_), xdim, ydim, zdim, 1))\n",
    "        print(\"Finished reading data: {0} training/test and {1} validation events\".format(len(x_),len(xval_)))\n",
    "        return x_,y_,xval_,yval_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to read the data from multiple files\n",
    "def read_NEW_data(fname):\n",
    "    \"\"\"Reads data from the specified file.\"\"\"\n",
    "    \n",
    "    # read in the signal events.\n",
    "    print(\"Reading real data...\")\n",
    "    d_array = []; d_evtnums = []\n",
    "    d_vox = h5py.File(fname,'r')\n",
    "    for trklbl in d_vox:\n",
    "        trk = d_vox[trklbl]\n",
    "        d_evtnums.append(int(trklbl[3:]))\n",
    "        evt = np.zeros([xdim, ydim, zdim])\n",
    "        for xx,yy,zz,ee in zip(trk[0],trk[1],trk[2],trk[3]):\n",
    "            evt[int(xx)][int(yy)][int(zz)] += ee\n",
    "        evt /= np.sum(evt)\n",
    "        d_array.append(evt)\n",
    "\n",
    "    d_array = np.array(d_array)\n",
    "    d_evtnums = np.array(d_evtnums)\n",
    "    print(\"done.\")\n",
    "\n",
    "    # reshape for training with TensorFlow        \n",
    "    print(\"Reshaping...\")\n",
    "    x_ = np.reshape(d_array, (len(d_array), xdim, ydim, zdim, 1))\n",
    "    \n",
    "    print(\"Finished reading data: {0} events\".format(len(x_)))\n",
    "    return x_,d_evtnums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network models\n",
    "These functions should define and return a Keras model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Define more neural networks here\n",
    "\n",
    "def model_3D(inputs):\n",
    "    \n",
    "    cinputs = Conv3D(64, (3, 3, 3), padding='valid', strides=(1, 1, 1), activation='relu',kernel_initializer='glorot_normal', kernel_regularizer=l2(0.000001))(inputs)\n",
    "    cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='same', data_format=None)(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-10, axis=-1, momentum=0.2, weights=None, gamma_regularizer=None, beta_regularizer=None, beta_initializer=\"zero\", gamma_initializer=\"one\")(cinputs)\n",
    "    cinputs = Conv3D(64, (3, 3, 3), padding='valid', strides=(1, 1, 1), activation='relu',kernel_initializer='glorot_normal', kernel_regularizer=l2(0.000001))(cinputs)\n",
    "    cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='same', data_format=None)(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=4, momentum=0.8, weights=None, gamma_regularizer=None, beta_regularizer=None, beta_initializer=\"zero\", gamma_initializer=\"one\")(cinputs)\n",
    "    cinputs = Conv3D(128, (2, 2, 2), padding='valid', strides=(1, 1, 1), activation='relu',kernel_initializer='glorot_normal', kernel_regularizer=l2(0.000001))(cinputs)\n",
    "    cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='same', data_format=None)(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=4, momentum=0.99, weights=None, gamma_regularizer=None, beta_regularizer=None, beta_initializer=\"zero\", gamma_initializer=\"one\")(cinputs)\n",
    "    cinputs = Conv3D(128, (2, 2, 2), padding='valid', strides=(1, 1, 1), activation='relu',kernel_initializer='glorot_normal', kernel_regularizer=l2(0.000001))(cinputs)\n",
    "    cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='same', data_format=None)(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=4, momentum=0.99, weights=None, gamma_regularizer=None, beta_regularizer=None, beta_initializer=\"zero\", gamma_initializer=\"one\")(cinputs)\n",
    "    f1 = Flatten()(cinputs)\n",
    "    f1 = Dense(units=256, activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=l2(0.000001))(f1)\n",
    "    f1 = Dropout(.7)(f1)\n",
    "\n",
    "    inc_output = Dense(units=1, activation='sigmoid', kernel_initializer='normal', kernel_regularizer=l2(0.000001))(f1)\n",
    "    model = Model(inputs, inc_output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=Nadam(lr=0.0002, beta_1=0.9, beta_2=0.999,\n",
    "                                epsilon=1e-08, schedule_decay=0.01), metrics=['accuracy'])\n",
    "    \n",
    "    # ----------\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a fully-connected neural network with 64 hidden neurons and 1 readout neuron\n",
    "def model_FC(inputs):\n",
    "    \n",
    "    f1 = Flatten()(inputs)\n",
    "    f1 = Dense(units=64, kernel_initializer=\"normal\", activation=\"sigmoid\")(f1)\n",
    "    inc_output = Dense(units=1, kernel_initializer=\"normal\", activation=\"sigmoid\")(f1)\n",
    "    model = Model(inputs, inc_output)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Nadam(lr=0.0001, beta_1=0.9, beta_2=0.999,\n",
    "                                  epsilon=1e-08, schedule_decay=0.01),\n",
    "                                  metrics=['accuracy'])  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data from training_data_1FWHM_25k.npz...\n"
     ]
    }
   ],
   "source": [
    "# read in the training data\n",
    "training_data_file = \"training_data_1FWHM_25k.npz\"\n",
    "if(os.path.isfile(training_data_file)):\n",
    "    print(\"Reading training data from {}...\".format(training_data_file))\n",
    "    tfile = np.load(training_data_file)\n",
    "    x_train = tfile['x_train']\n",
    "    x_val   = tfile['x_val']\n",
    "    y_train = tfile['y_train']\n",
    "    y_val   = tfile['y_val']\n",
    "else:\n",
    "    x_train, y_train, x_val, y_val = read_MC(data_train_sg, data_train_bg, 25000, 0.1)\n",
    "    np.savez_compressed(training_data_file, x_train=x_train, y_train=y_train, x_val=x_val, y_val=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train the DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jrenner/miniconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/jrenner/miniconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/jrenner/miniconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 30, 30, 30, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 28, 28, 28, 64)    1792      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 14, 14, 14, 64)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 14, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 12, 12, 12, 64)    110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 6, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 6, 6, 64)       256       \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 5, 5, 5, 128)      65664     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 3, 3, 3, 128)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 3, 3, 128)      512       \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 2, 2, 2, 128)      131200    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 1, 1, 128)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1, 1, 1, 128)      512       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 344,129\n",
      "Trainable params: 343,361\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# set load_model to true and specify the file to load in a previously defined/trained model\n",
    "load_weights = False\n",
    "mfile = 'models/conv3d_classifier_0.4489-0.4916.h5'\n",
    "\n",
    "if(load_weights):\n",
    "    model = load_model(mfile)\n",
    "else:\n",
    "    \n",
    "    # otherwise define the model\n",
    "    inputs = Input(shape=(xdim, ydim, zdim, 1))\n",
    "    model = model_3D(inputs)\n",
    "    \n",
    "# define callbacks (actions to be taken after each epoch of training)\n",
    "file_lbl = \"{epoch:02d}-{loss:.4f}-{val_loss:.4f}\"\n",
    "filepath=\"weights-{0}.h5\".format(file_lbl)\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "tboard = callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n",
    "lcallbacks = [checkpoint, tboard]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "hist = model.fit(x_train, y_train, shuffle=True, epochs=200, batch_size=200, verbose=1, validation_data=(x_val,y_val), callbacks=lcallbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show signal efficiency vs. background rejection for trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the predictions\n",
    "loss_and_metrics = model.evaluate(x_val, y_val);\n",
    "y_pred = model.predict(x_val, batch_size=100, verbose=0)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists of values for signal vs. background curve\n",
    "npoints = 200\n",
    "fname_svsb = \"plt/plt_val.h5\"\n",
    "bg_rej = []; si_eff = []\n",
    "print(\"-- Calculating points...\")\n",
    "for thh in np.arange(0,1,1./npoints):\n",
    "    nts = 0; ntb = 0\n",
    "    ncs = 0; ncb = 0\n",
    "    for ye,yp in zip(y_val,y_pred):\n",
    "        if(ye == 0):\n",
    "            ntb += 1  # add one background event\n",
    "            if(yp < thh):\n",
    "                ncb += 1  # add one correctly predicted background event\n",
    "\n",
    "        if(ye == 1):\n",
    "            nts += 1  # add one signal event\n",
    "            if(yp >= thh):\n",
    "                ncs += 1  # add one correctly predicted signal event\n",
    "                \n",
    "    si_eff.append(1.0*ncs/nts)\n",
    "    bg_rej.append(1.0*ncb/ntb)\n",
    "    #print(\"-- {0} of {1} ({2}%) correct background events; {3} of {4} ({5}%) correct signal events\".format(ncb,ntb,1.0*ncb/ntb*100,ncs,nts,1.0*ncs/nts*100))\n",
    "\n",
    "# save the results to file\n",
    "print(\"-- Saving results...\")\n",
    "si_eff = np.array(si_eff); bg_rej = np.array(bg_rej)\n",
    "f = tb.open_file(fname_svsb, 'w')\n",
    "filters = tb.Filters(complib='blosc', complevel=9, shuffle=False)\n",
    "\n",
    "atom    = tb.Atom.from_dtype(si_eff.dtype)\n",
    "sarr    = f.create_earray(f.root, 'si_eff', atom, (0, npoints), filters=filters)\n",
    "sarr.append([si_eff])\n",
    "\n",
    "atom    = tb.Atom.from_dtype(bg_rej.dtype)\n",
    "sarr    = f.create_earray(f.root, 'bg_rej', atom, (0, npoints), filters=filters)\n",
    "sarr.append([bg_rej])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot signal vs. background curves\n",
    "fnames = [\"plt/plt_val.h5\"]\n",
    "labels = [\"Validation data\"]\n",
    "colors = [\"black\"]\n",
    "\n",
    "# set up the plot\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(5.0)\n",
    "fig.set_figwidth(7.5)\n",
    "\n",
    "for nm,lb,co in zip(fnames,labels,colors):\n",
    "    \n",
    "    # read in the signal efficiency vs. background rejection information\n",
    "    fn = tb.open_file(nm,'r')\n",
    "    eff = fn.root.si_eff[0]\n",
    "    bgr = fn.root.bg_rej[0]\n",
    "    \n",
    "    plt.plot(eff,bgr,color=co,label=lb,lw=2)\n",
    "    fn.close()\n",
    "    \n",
    "plt.xlabel(\"signal efficiency\")\n",
    "plt.ylabel(\"background rejection\")\n",
    "plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on NEW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real, evt_real = read_NEW_data(\"/data/analysis/datasets/15x15x15/voxels_combined_4739.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = model.predict(x_real, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npred_signal = sum(y_real > 0.205)\n",
    "npred_background = sum(y_real <= 0.205)\n",
    "print(\"Number of predicted signal events = {}, background events = {}\".format(npred_signal,npred_background))\n",
    "plt.hist(y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the arrays to a file.\n",
    "ysave = y_real.reshape(len(y_real))\n",
    "esave = evt_real.reshape(len(evt_real))\n",
    "np.savez(\"classification_4739.npz\",evtnum=esave,y=ysave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
